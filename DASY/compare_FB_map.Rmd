---
title: "Compare DASY with Facebook"
author: "Quentin D. Read"
date: "5/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Summary

We are interested in whether our dasymetric map adds value to existing data. Facebook(TM) created a similar data product, not using their data but using some kind of imagery. A vague description of the method seems to suggest that it's similar to the one we used. I wanted to visualize any discrepancy between Facebook's data and ours.

Note the bash scripts in here are not run when the notebook is created, I just include them here for later reproducibility. Only the plotting and data comparison in R is run when the notebook is created.

## Download and ready the Facebook data

The data are hosted at a page on [data.humdata.org](# https://data.humdata.org/dataset/united-states-high-resolution-population-density-maps-demographic-estimates).
This code  downloads all the data. I put the URLs of all the map tiles in a text file. This bash script loops through the files and downloads them all, then unzips them. I put them in a folder called `/nfs/public-data/Facebook_pop_density` so they can be accessed by anyone on the SESYNC server.

```{bash, eval = FALSE}
urls="/nfs/qread-data/temp/fbpopfileurls.txt"
direc="/nfs/qread-data/raw_data/facebookpop"

# Download
while read url; do
  echo "$url"
  wget -P $direc $url
done < $urls

# Unzip
for file in $direc/*.zip; do
  unzip $file
done
```

A virtual raster (.vrt) file is already provided in the directory with all the tiles mosaicked together.

## Example: Anne Arundel County

Unfortunately our dasymetric map is not the most recent, being based on a combination of 2010 and 2016 data. The facebook map purports to be 2019 population but I haven't yet figured out the datasets they are using to get that. But some discrepancies between the two maps might come from discrepancies in the two time points.

We will use GDAL to crop the virtual raster for the entire country to just AA county boundaries so we can read in all its pixel values easily. First create a temporary file of AA county boundaries from the full USA county boundaries shapefile. Also transform it to the unprojected CRS of the facebook raster. 

```{bash, eval = FALSE}
ogr2ogr -overwrite -where fips="24003" /nfs/qread-data/temp/aacounty.gpkg /nfs/qread-data/raw_data/landuse/USA/USA_county_2014_aea.gpkg
raster_proj=`gdalsrsinfo $direc/population_usa_2019-07-01.vrt -o proj4 | xargs`
ogr2ogr -overwrite -f "GPKG" -t_srs "${raster_proj}" /nfs/qread-data/temp/aacountylonglat.gpkg /nfs/qread-data/temp/aacounty.gpkg
```

Now crop the virtual raster to AA county boundaries (code not run in notebook). 

```{bash, eval = FALSE}
gdalwarp /nfs/qread-data/raw_data/facebookpop/population_usa_2019-07-01.vrt /nfs/qread-data/temp/aa_pop_fb_longlat.tif \
  -overwrite -crop_to_cutline -cutline /nfs/qread-data/temp/aacountylonglat.gpkg
gdalwarp /nfs/rswanwick-data/DASY/tifs/neon-dasy-24-003.tif /nfs/qread-data/temp/aa_pop_dasy.tif \
  -tr 30 30 -overwrite -crop_to_cutline -cutline /nfs/qread-data/temp/aacountylonglat.gpkg
```

At first, I tried to tell the difference between the two rasters by projecting the Facebook raster (which is long-lat and not equal area) to the equal area projection of the dasymetric raster, and then pixel-wise subtracting one raster from the other. This turned out to be problematic because the reprojection unavoidably causes a pretty big error, changing the total population gotten from the pixel sums by about 100,000. So instead, I decided to just plot both of them next to each other to see what they both look like, for the "eyeball test," then extract a large number of point values from each of the rasters on an even grid. We can look at what discrepancies there are between those point values.

From now on the code is in R and is run when the notebook is rendered.

### Load the data into R

```{r load data, message = FALSE, warning = FALSE}
library(stars)
library(sf)
library(dplyr)
library(purrr)
library(ggplot2)

theme_set(theme_bw())

aadasy <- read_stars('/nfs/qread-data/temp/aa_pop_dasy.tif')
aafb <- read_stars('/nfs/qread-data/temp/aa_pop_fb_longlat.tif')
aabounds <- st_read('/nfs/qread-data/temp/aacounty.gpkg')
aabounds_longlat <- st_read('/nfs/qread-data/temp/aacountylonglat.gpkg')
```

### Inspect data

Look at projection info. The Facebook raster is still in unprojected coordinates.

```{r projections}
st_crs(aadasy)$proj4string
st_crs(aafb)$proj4string
st_crs(aabounds)$proj4string
```

What are the total populations for each raster?

```{r totals}
map(aadasy, sum, na.rm = TRUE)

map(aafb, sum, na.rm = TRUE)
```

The dasymetric map has a slightly higher sum which is strange because the Facebook map is supposed to be based on a few years later and the county population should be growing. But they are pretty close.

Now for the eyeball test: look at a plot of each raster.

```{r initial plot}
fill_scale <- scale_fill_viridis_c(na.value = 'transparent', name = 'population')

ggplot() +
  geom_stars(data = aadasy) +
  geom_sf(data = st_geometry(aabounds), fill = NA) +
  theme(legend.position = 'bottom') +
  fill_scale +
  ggtitle('our dasymetric map')

ggplot() +
  geom_stars(data = aafb) +
  geom_sf(data = st_geometry(aabounds_longlat), fill = NA) +
  theme(legend.position = 'bottom') +
  fill_scale +
  ggtitle('facebook dasymetric map')
```

The dasymetric raster looks like it has denser coverage of non-missing pixels. The Facebook raster has some pixels with much higher population values, I suppose because the same number of people are represented as being squeezed into fewer pixels. Overall, though, it's fairly hard to do any kind of meaningful "eyeball test" from these maps because so many of the pixels have very low values and most of the population is clustered into a few high-value pixels sparsely scattered around.

### Systematically compare the two estimates

Now, to sample each raster at the same grid points, we generate an even 500-m grid over AA county and trim it to the bounds of the county. We get a bit less than 5000 points to compare. Only some of these points will end up having data once we do the extraction.

```{r grid}
aagrid <- st_make_grid(aabounds, cellsize = 500, what = 'centers')
aagrid <- aagrid[aabounds]

ggplot() + 
  geom_sf(data = aabounds, fill = NA) + 
  geom_sf(data = aagrid, size = 0.2)
```

Extract the values from both rasters at each point. Points need to be transformed to lat-long to extract from the Facebook raster. 

```{r extract values}
aadasy_points <- aggregate(aadasy, aagrid, function(x) x[1], as_points = FALSE) %>%
  st_as_sf %>%
  rename(pop_dasy = aa_pop_dasy.tif)

aagrid_longlat <- st_transform(aagrid, st_crs(aafb))

aafb_points <- aggregate(aafb, aagrid_longlat, function(x) x[1], as_points = FALSE) %>%
  st_as_sf %>%
  st_transform(st_crs(aadasy_points)) %>%
  rename(pop_fb = aa_pop_fb_longlat.tif)
```

How many non-missing points do we have for both rasters and how many of these overlap?

```{r nonmissing counts}
sum(!is.na(aadasy_points$pop_dasy))
sum(!is.na(aafb_points$pop_fb))
sum(!is.na(aadasy_points$pop_dasy) & !is.na(aafb_points$pop_fb))
```

About 400 overlap, which is plenty to compare.

Make maps of the extracted values. Put the two color scales into the same range so that we can more easily compare (do this by finding the min and max of each raster's values).

```{r map extracted values}
scale_range <- ceiling(range(c(aadasy_points$pop_dasy, aafb_points$pop_fb), na.rm = TRUE))

color_scale <- scale_color_viridis_c(name = 'population', limits = scale_range)

ggplot() + 
  geom_sf(data = aabounds, fill = NA) + 
  geom_sf(data = aadasy_points %>% filter(!is.na(pop_dasy)), aes(color = pop_dasy), size = 0.8) +
  color_scale +
  ggtitle('our dasymetric map') +
  theme(legend.position = 'bottom')

ggplot() + 
  geom_sf(data = aabounds, fill = NA) + 
  geom_sf(data = aafb_points %>% filter(!is.na(pop_fb)), aes(color = pop_fb), size = 0.8) +
  color_scale +
  ggtitle('facebook dasymetric map') +
  theme(legend.position = 'bottom')
```

At this point in a previous version of this document I did a correction for unequal pixel sizes but after further reflection I decided that it was not necessary and only made things worse. So I've taken it out. But it is important to note that there is a possible additional reason for a discrepancy between the values: the projected equal-area dasymetric raster has pixel sizes of 30x30 m while the lat-long unprojected raster Facebook produced has approximately 31x24 m pixels (that's about 83% the area). 

### Final comparison plots

Now we can take the difference in population densities between the two rasters and plot. Here I show a scatterplot with the 1:1 line for reference, which shows there is a wide discrepancy between estimates. The density plot shows the distribution of differences with a reference line at zero.
The majority have a negative value (Facebook's estimate is higher at most pixels).

```{r difference and plot}
aadasy_points <- aadasy_points %>%
  mutate(pop_fb = aafb_points$pop_fb,
         pop_diff = pop_dasy - pop_fb)

ggplot(aadasy_points, aes(x = pop_dasy, y = pop_fb)) +
  geom_point() +
  geom_abline(slope = 1, linetype = 'dashed', size = 1, color = 'slateblue') +
  theme(aspect.ratio = 1) +
  labs(x = 'our dasymetric', y = 'facebook dasymetric')

ggplot(aadasy_points, aes(x = pop_dasy - aafb_points$pop_fb)) +
  geom_density() +
  geom_vline(xintercept = 0, linetype = 'dashed', color = 'slateblue') +
  labs(x = 'our dasymetric - facebook dasymetric')
```

Let's map the locations where the differences are positive versus negative and see if there is any pattern in where our values are higher (positive difference, blue points) versus Facebook's (negative difference, red points). I also plot the location of cities for reference.

```{r map the differences}
data(us.cities, package = 'maps')

us.cities <- us.cities %>% 
  st_as_sf(coords = c('long', 'lat'), crs = 4326)

aacity <- st_join(us.cities, aabounds_longlat, left = FALSE) %>%
  mutate(name = gsub(' MD', '', name))

ggplot() + 
  geom_sf(data = aabounds, fill = NA) + 
  geom_sf(data = aadasy_points %>% filter(!is.na(pop_diff)), aes(color = pop_diff > 0), size = 0.8) +
  geom_sf(data = aacity, size = 3, alpha = 0.5) +
  geom_sf_text(data = aacity, aes(label = name), vjust = -1, size = 4, fontface = 'bold') +
  scale_color_manual(values = c('indianred', 'slateblue')) +
  ggtitle('population estimate differences') +
  theme(legend.position = 'bottom')
```

At first it looked to me like the areas where our estimates are higher are concentrated in the cities/towns of Anne Arundel County but maybe there is no such pattern?

## Aggregating back up to block group

To further compare, let's sum up the population rasters by block group. Ours is based on the ACS 2016 census block group population estimates so if we aggregate our raster by that geometry it should give identical population totals to the block groups. The Facebook one should equal the 2019 estimates.

Use `tidycensus` to get the population estimates.

```{r}
library(tidycensus)

census_api_key(readLines('~/censusapikey.txt'))

aapop2016 <- get_acs(geography = "block group", variables = "B01003_001", 
                     year = 2016, state = '24', county = '003', 
                     geometry = TRUE)   

aapop2019 <- get_acs(geography = "block group", variables = "B01003_001", 
                     year = 2019, state = '24', county = '003', 
                     geometry = TRUE)   

aapop2016_proj <- st_transform(aapop2016, st_crs(aadasy))
aapop2019_proj <- st_transform(aapop2019, st_crs(aafb))

```

Aggregate the rasters by blockgroup polygon.

```{r}
aadasy_block <- aggregate(aadasy, aapop2016_proj, sum, na.rm = TRUE) %>%
  st_as_sf

aafb_block <-aggregate(aafb, aapop2019_proj, sum, na.rm = TRUE) %>%
  st_as_sf

aapop2016 <- aapop2016 %>%
  mutate(pop_dasy = aadasy_block$aa_pop_dasy.tif)

aapop2019 <- aapop2019 %>%
  mutate(pop_fb = aafb_block$aa_pop_fb_longlat.tif)
```
How do they compare? Ours reproduces the block group populations exactly, as expected.

```{r}
ggplot(aapop2016, aes(x = estimate, y = pop_dasy)) +
  geom_point() +
  geom_abline(slope = 1, linetype = 'dashed', color = 'slateblue') +
  labs(x = 'ACS 2016 estimate', y = 'our map aggregated') +
  theme(aspect.ratio = 1)
```

The Facebook map does not so it must be based on different input data. But given that the Census data itself is also an estimate, this doesn't necessarily mean the Facebook estimate is "wrong." It may be closer to the truth after all.

```{r}
ggplot(aapop2019, aes(x = estimate, y = pop_fb)) +
  geom_point() +
  geom_abline(slope = 1, linetype = 'dashed', color = 'slateblue') +
  labs(x = 'ACS 2019 estimate', y = 'FB map aggregated') +
  theme(aspect.ratio = 1)
```

Make a map showing the deviation between Facebook estimates and Census 2019 estimates. Somehow the facebook population greatly underestimates a block group north of Pasadena which has ~8000 people but is only estimated to have ~3000 by facebook.

```{r}
aapop2019 <- mutate(aapop2019, pop_diff = estimate - pop_fb)

ggplot(aapop2019) +
  geom_sf(aes(fill = pop_diff)) +
  geom_sf_text(data = aacity, aes(label = name), size = 4, fontface = 'bold') +
  scico::scale_fill_scico(palette = 'vik', begin = 0.4, end = 1, 
                          name = 'census pop - facebook pop', breaks = c(0, 2500, 5000)) +
  theme(legend.position = 'bottom')
```


## Conclusion

In summary, these two estimates have a lot of differences. There are three main possible reasons I could think of.

- **One or both of them is wrong.** Hopefully this isn't the case and there is not too much to do about it anyway because we don't have access to the "true" values.
- **Different algorithms used to produce the two maps.** This is likely to be the case but I need to do some digging to find what method Facebook used.
- **Different input data.** Our estimate uses a combination of 2010 and 2016 data while Facebook's is supposedly for 2019. So that would explain a bit of the discrepancy. But also related to the above point, the two algorithms might use completely different kinds of input data so again I'd have to dig and find that out. As we can see from the block group aggregation, our estimate is constrained to give the census population totals by block group while Facebook's is not. So likely Facebook does not use that dataset to produce their estimates.

Later I will do this for multiple counties in different parts of the country to see whether there are systematic differences in the discrepancies between our dataset and Facebook's. For example the error might be systematically higher in urban areas, or rural. I have no idea.

## References

Kelly did some digging and found a couple of papers that may give some hints about the method Facebook is using. Here are links to them:

- [preprint on arXiv describing the building detection algorithm](https://arxiv.org/abs/1712.05839)
- [Nat Comms paper which uses the data and describes the method briefly](https://www.nature.com/articles/s41467-019-09282-y)
- [paper on dasymetric mapping using addresses](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9671.2011.01270.x) - this is a bit of a different method but may be useful reading.
