---
title: "Risk estimate figure code: V2.0"
author: "Quentin D. Read"
date: "7/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This document has all the code to make the data underlying the "validation" or inference figure, which will give a visual answer to the question of how much using dasymetric population estimates actually changes our inference. This is the second version, which includes two different case studies: wildfire risk in counties in the western USA, and flood risk in coastal counties. For the first example I am using the Forest Service's wildfire hazard map that classifies the contiguous USA into 5 fire risk categories, at 270-meter pixel size. For the second example I am using the flood risk maps provided by FEMA, which include the flood water surface elevation (WSE) for a 100-year flood event at a 10-meter pixel resolution.

The procedure is:

- take a stratified random sample of the counties of interest, stratified by population.
- clip population rasters and environmental rasters to study area boundaries (we're using a few selected counties). 
- convert the environmental rasters to polygons, merging all adjacent pixels with the same class
- for each polygon, sum the dasymetric population pixels in that polygon to get numbers of people in that risk category
- sum across risk classes to get a histogram for each county
- compare the histograms for each county side-by-side to see if there is a difference in inference between population estimates

The population estimates we're using for now are:

- Our dasymetric estimate
- EPA dasymetric estimate
- Facebook population estimate
- Huang et al. population grid
- Census block groups with population spread equally across block group

*Note*: some of this code is not run when this notebook is rendered because it takes a long time. I ran it earlier and saved the output to load when the notebook is rendered.

## Stratified random sample of counties

I took the SRS of counties in a separate script. It isn't too important which counties are used because this is just illustrative. For the wildfire risk, I took all counties in the eleven Western states (WA, OR, CA, ID, NV, MT, WY, CO, UT, AZ, NM) and divided them up by population quintile (5 groups). I selected three random counties from each quintile. I did the same for flood risk except I used only coastal states in the lower 48 (both Atlantic and Pacific coasts). I had to sample quite a few counties to get to 10 counties that actually had flood risk rasters available from FEMA, so they are not perfectly divided among the quintiles. But there is a decent spread of population.

The wildfire risk map is available for the entire contiguous USA but the FEMA flood risk maps are only available for some of the counties. I went to the FEMA site and searched each county manually to get the data. I got data for ten of the counties which should be plenty for our case study.

What counties are we using?

```{r, echo = FALSE}
counties_wildfire <- read.csv('/nfs/qread-data/DASY/counties_wildfire.csv', colClasses = 'character')
counties_flood <- read.csv('/nfs/qread-data/DASY/counties_flood_allfematifs.csv', colClasses = 'character')

knitr::kable(counties_wildfire[,1:2], col.names = c('County', 'State'), caption = 'Wildfire risk counties')

counties_flood$county_name <- paste(counties_flood$county_name, 'County')

knitr::kable(counties_flood[,2:1], col.names = c('County', 'State'), caption = 'Flood risk counties')
```

## Generate separate shapefiles for each target county

This makes it a lot easier to do operations on each county separately. We also project them to lat-long geographic coordinate system for use with the Facebook raster.

```{r, message = FALSE, warning = FALSE}
library(stars)
library(sf)
library(purrr)
library(dplyr)
library(glue)
library(tidycensus)
library(readr)
library(ggplot2)
library(gdalUtils)
library(rslurm)
```

Here note that the FIPS code has to be put in single quotes for the SQL WHERE. In contrast, when running from the command line I had to put it in double quotes. That took a lot of trial and error to figure out!

```{r, eval = FALSE}
gpkgpath <- "/nfs/qread-data/DASY/countybounds"
raster_proj <- gdalsrsinfo('/nfs/qread-data/raw_data/facebookpop/population_usa_2019-07-01.vrt', as.CRS = TRUE)

all_fips <- bind_rows(counties_flood, counties_wildfire) %>%
  select(state_code, county_code)

pwalk(all_fips, function(state_code, county_code) {
  fips <- paste0(state_code, county_code)
  # Extract only one county from the all county geopackage
  ogr2ogr(src_datasource_name = '/nfs/qread-data/raw_data/landuse/USA/USA_county_2014_aea.gpkg',
          dst_datasource_name = glue('{gpkgpath}/county{fips}.gpkg'),
          overwrite = TRUE, f = 'GPKG', where = glue('fips=\'{fips}\''))
  # Project the single county GPKG to the geographic CRS used by Facebook
  ogr2ogr(src_datasource_name = glue('{gpkgpath}/county{fips}.gpkg'),
          dst_datasource_name = glue('{gpkgpath}/county{fips}longlat.gpkg'),
          overwrite = TRUE, f  = 'GPKG', t_srs = raster_proj)
})
```

## Clip rasters to study area boundaries

Our dasymetric rasters and the flood risk rasters are already split up by county. So here it's just the wildfire raster, the EPA dasymetric raster, the Facebook raster, and the Huang et al. population grid that are being split up. Also ensure that our dasymetric raster matches the long-lat geographic CRS by creating a new version.

```{r, eval = FALSE}
tifpath <- "/nfs/qread-data/DASY/countyrasters"
gpkgpath <- "/nfs/qread-data/DASY/countybounds"

pwalk(all_fips, function(state_code, county_code) {
  fips <- paste0(state_code, county_code)
  # Wildfire
  gdalwarp(srcfile = '/nfs/qread-data/DASY/whp/Data/whp2020_GeoTIF/whp2020_cls_conus.tif',
           dstfile = glue('{tifpath}/county{fips}_wildfire.tif'),
           overwrite = TRUE, crop_to_cutline = TRUE,
           cutline = glue('{gpkgpath}/county{fips}.gpkg'))
  # EPA Dasymetric
  gdalwarp(srcfile = '/nfs/qread-data/DASY/epadasy/dasymetric_us_20160208/dasymetric_us_20160208.tif',
           dstfile = glue('{tifpath}/county{fips}_epadasy.tif'),
           overwrite = TRUE, crop_to_cutline = TRUE,
           cutline = glue('{gpkgpath}/county{fips}.gpkg'))
  # Facebook Dasymetric
  gdalwarp(srcfile = '/nfs/qread-data/raw_data/facebookpop/population_usa_2019-07-01.vrt',
           dstfile = glue('{tifpath}/county{fips}_fblonglat.tif'),
           overwrite = TRUE, crop_to_cutline = TRUE,
           cutline = glue('{gpkgpath}/county{fips}longlat.gpkg'))
  # Huang Dasymetric
  gdalwarp(srcfile = '/nfs/qread-data/DASY/huang_grid/PopGrid.tif',
           dstfile = glue('{tifpath}/county{fips}_huangdasy.tif'),
           overwrite = TRUE, crop_to_cutline = TRUE,
           cutline = glue('{gpkgpath}/county{fips}.gpkg'))
  # Our dasymetric, crop to cutline with the longlat object
  gdalwarp(srcfile = glue('/nfs/qread-data/DASY/tifs/neon-dasy-{state_code}-{county_code}.tif'),
           dstfile = glue('{tifpath}/county{fips}_dasy.tif'),
           overwrite = TRUE, crop_to_cutline = TRUE, tr = c(30, 30),
           cutline = glue('{gpkgpath}/county{fips}longlat.gpkg'))
})
```

## Read data

Loop through and read the rasters for each county. 

```{r, message = FALSE, warning = FALSE, results = "hide"}
counties_wildfire <- counties_wildfire %>% mutate(fips = paste0(state_code, county_code))
tifpath <- '/nfs/qread-data/DASY/countyrasters'
dasypath <- '/nfs/qread-data/DASY/tifs'

wildfire_rasters <- map(counties_wildfire$fips, ~ read_stars(glue('{tifpath}/county{.}_wildfire.tif')))
ourdasy_rasters_wf <- map(counties_wildfire$fips, ~ read_stars(glue('{dasypath}/neon-dasy-{substr(., 1, 2)}-{substr(., 3, 5)}.tif')))
epadasy_rasters_wf <- map(counties_wildfire$fips, ~ read_stars(glue('{tifpath}/county{.}_epadasy.tif')))
huang_rasters_wf <- map(counties_wildfire$fips, ~ read_stars(glue('{tifpath}/county{.}_huangdasy.tif')))
fbpop_rasters_wf <- map(counties_wildfire$fips, ~ read_stars(glue('{tifpath}/county{.}_fblonglat.tif')))

ourdasy_rasters_fl <- map(counties_flood$fips, ~ read_stars(glue('{dasypath}/neon-dasy-{substr(., 1, 2)}-{substr(., 3, 5)}.tif')))
epadasy_rasters_fl <- map(counties_flood$fips, ~ read_stars(glue('{tifpath}/county{.}_epadasy.tif')))
huang_rasters_fl <- map(counties_flood$fips, ~ read_stars(glue('{tifpath}/county{.}_huangdasy.tif')))
fbpop_rasters_fl <- map(counties_flood$fips, ~ read_stars(glue('{tifpath}/county{.}_fblonglat.tif')))
```

Also load the census block group population estimates and geographies. These will be used for the "naive equal weighting" population estimate. Note that all the flood rasters have a different coordinate reference system so the block group population polygons are transformed each to a different one.

```{r, message = FALSE, eval = FALSE}
get_blockgroup_pop <- function(FIPS, crs) {
 
  pop <- get_acs(geography = "block group", variables = "B01003_001", 
                 year = 2016, state= substr(FIPS, 1, 2), county = substr(FIPS, 3, 5), 
                 geometry = TRUE)   
  
  # Data QC: remove empty geometries from pop
  pop <- pop[!is.na(st_dimension(pop)), ]
  
  st_transform(pop, crs = crs)
}

census_api_key(readLines('/research-home/qread/censusapikey.txt')) 
crs_wildfire <- st_crs(wildfire_rasters[[1]])
blockgroup_pops_wf <- map(counties_wildfire$fips, get_blockgroup_pop, crs = crs_wildfire)

crs_flood <- map(flood_rasters, st_crs)
blockgroup_pops_fl <- map2(counties_flood$fips, crs_flood, get_blockgroup_pop)
```


## "Polygonize" the environmental rasters

We will use `st_as_sf()` to convert the wildfire raster and flood raster (converted to binary) to polygons. I needed to parallelize this because of the high memory requirements.

```{r, eval = FALSE}
sjob_wild <- slurm_map(wildfire_rasters, st_as_sf, jobname = 'wf_to_poly', 
                       nodes = 2, cpus_per_node = 4,
                       as_points = FALSE, merge = TRUE)
wildfire_polygons <- get_slurm_out(sjob_wild)
cleanup_files(sjob_wild)

make_poly_and_raster <- function(wse_filename, fips) {
  flood_raster <- raster(wse_filename)
  file_name <- glue('/nfs/qread-data/DASY/countyrasters/county{fips}_floodbinary.tif')
  if (!file.exists(file_name)) {
    flood_notna_raster <- st_as_stars(!is.na(flood_raster))
    write_stars(flood_notna_raster, file_name)
  } else {
    flood_notna_raster <- read_stars(file_name)
  }
  flood_notna_poly <- st_as_sf(flood_notna_raster, as_points = FALSE, merge = TRUE)
  return(flood_notna_poly)
}

sjob_flood <- slurm_apply(make_poly_and_raster, counties_flood[,c('wse_filename', 'fips')],
                          pkgs = c('raster', 'stars', 'sf', 'glue'), jobname = 'fl_to_poly',
                          nodes = 4, 
                          slurm_options = list(partition = 'sesync', mem = '100gb'))

flood_polygons <- get_slurm_out(sjob_flood)
cleanup_files(sjob_flood)

saveRDS(wildfire_polygons, file = '/nfs/qread-data/DASY/wildfire_polygons.RDS')
saveRDS(flood_polygons, file = '/nfs/qread-data/DASY/flood_polygons.RDS')
```

Convert the polygons to each of the rasters' coordinate reference systems. Correct invalid geometries in the lat-long polygons.

```{r}
wildfire_polygons <- readRDS('/nfs/qread-data/DASY/wildfire_polygons.RDS')
flood_polygons <- readRDS('/nfs/qread-data/DASY/flood_polygons.RDS')

wildfire_polygons_ourdasycrs <- map(wildfire_polygons, st_transform, crs = st_crs(ourdasy_rasters[[1]]))
wildfire_polygons_epadasycrs <- map(wildfire_polygons, st_transform, crs = st_crs(epadasy_rasters[[1]]))
wildfire_polygons_huangcrs <- map(wildfire_polygons, st_transform, crs = st_crs(huang_rasters[[1]]))
wildfire_polygons_fbpopcrs <- map(wildfire_polygons, st_transform, crs = st_crs(fbpop_rasters[[1]])) %>%
  map(st_make_valid)

flood_polygons_ourdasycrs <- map(flood_polygons, st_transform, crs = st_crs(ourdasy_rasters[[1]]))
flood_polygons_epadasycrs <- map(flood_polygons, st_transform, crs = st_crs(epadasy_rasters[[1]]))
flood_polygons_huangcrs <- map(flood_polygons, st_transform, crs = st_crs(huang_rasters[[1]]))
flood_polygons_fbpopcrs <- map(flood_polygons, st_transform, crs = st_crs(fbpop_rasters[[1]])) %>%
  map(st_make_valid)

```


## Get population totals by polygon

Now for each of these polygons, sum up the number of individuals. Repeat for each of the population estimates. Again parallelize with rslurm.

```{r, eval = FALSE}
sjob_wild_sums <- slurm_apply(function(i) {
  wildfire_sums_ourdasy <- map2(ourdasy_rasters, wildfire_polygons_ourdasycrs, aggregate, FUN = sum, na.rm = TRUE)
  wildfire_sums_epadasy <- map2(epadasy_rasters, wildfire_polygons_epadasycrs, aggregate, FUN = sum, na.rm = TRUE)
  wildfire_sums_huang <- map2(huang_rasters, wildfire_polygons_huangcrs, aggregate, FUN = sum, na.rm = TRUE)
  wildfire_sums_fbpop <- map2(fbpop_rasters, wildfire_polygons_fbpopcrs, aggregate, FUN = sum, na.rm = TRUE)
})

wildfire_sums_ourdasy <- map2(ourdasy_rasters, wildfire_polygons_ourdasycrs, aggregate, FUN = sum, na.rm = TRUE)
wildfire_sums_epadasy <- map2(epadasy_rasters, wildfire_polygons_epadasycrs, aggregate, FUN = sum, na.rm = TRUE)
wildfire_sums_huang <- map2(huang_rasters, wildfire_polygons_huangcrs, aggregate, FUN = sum, na.rm = TRUE)
wildfire_sums_fbpop <- map2(fbpop_rasters, wildfire_polygons_fbpopcrs, aggregate, FUN = sum, na.rm = TRUE)

flood_sums_ourdasy <- map2(ourdasy_rasters, flood_polygons_ourdasycrs, aggregate, FUN = sum, na.rm = TRUE)
flood_sums_epadasy <- map2(epadasy_rasters, flood_polygons_epadasycrs, aggregate, FUN = sum, na.rm = TRUE)
flood_sums_huang <- map2(huang_rasters, flood_polygons_huangcrs, aggregate, FUN = sum, na.rm = TRUE)
flood_sums_fbpop <- map2(fbpop_rasters, flood_polygons_fbpopcrs, aggregate, FUN = sum, na.rm = TRUE)
```

For the wildfire by block group population weighting, we will do it the opposite way (population polygon and wildfire raster). Tabulate wildfire pixels for each block group polygon, and convert to a matrix. Then divide the population of each block group among the classes based on the number of pixels in each class in each block group. 

```{r, eval = FALSE}
table_by_polygon <- function(env_raster, bg_pop_poly) {
  map(st_geometry(bg_pop_poly), ~ as.data.frame(table(st_crop(env_raster, .)[[1]], useNA = 'always')))
}

wildfire_tables_blockgroups <- map2(wildfire_rasters, blockgroup_pops, table_by_polygon)

sum_classes_blockgroup <- function(pop_table, bg_poly) {
  map_dfr(1:length(pop_table), ~ data.frame(st_drop_geometry(bg_poly[., c('GEOID', 'estimate')]), pop_table[[.]])) %>%
    group_by(GEOID) %>%
    mutate(pop = estimate * Freq / sum(Freq))
}

wildfire_sums_blockgroups <- map2(wildfire_tables_blockgroups, blockgroup_pops, sum_classes_blockgroup)
```

The method for flood sums by block group is different as well. We are using the polygons in both cases.

```{r, eval = FALSE}
# For blockgroups, instead of aggregating pop raster by flood polygon, instead intersect blockgroup polygon and flood polygon.
get_blockgroup_sums <- function(bg_pop, env_poly, env_crs) {
  bg_pop %>%
    st_transform(env_crs) %>%
    mutate(area = st_area(.)) %>%
    st_intersection(env_poly) %>%
    mutate(area_int = st_area(.))
}

wse_sums_blockgroup <- map2(blockgroup_pops, wse_notna_poly, env_crs = st_crs(wse_notna_raster[[1]]))

```


```{r, echo = FALSE, eval = FALSE}
save(wildfire_sums_blockgroups, wildfire_sums_epadasy, wildfire_sums_ourdasy, wildfire_sums_huang, wildfire_sums_fbpop,
     flood_sums_blockgroups, flood_sums_epadasy, flood_sums_ourdasy, flood_sums_huang, flood_sums_fbpop,
     file = '/nfs/qread-data/DASY/allrisk_sums_temp.RData')
```


```{r, include = FALSE}
load('/nfs/qread-data/DASY/allrisk_sums_temp.RData')
```


## Get grand totals by class for each county

Join the individual polygon population sums with the polygon classes and get the grand totals by class for each county, combining the output into one dataframe. 

```{r, message = FALSE}
grandtotal_classes <- function(env_poly, pop_raster) {
  data.frame(env_class = env_poly[[1]], pop = pop_raster[[1]]) %>%
    group_by(env_class) %>%
    summarize(pop = sum(pop))
}

wildfire_grandtotals_ourdasy <- map2(wildfire_polygons_ourdasycrs, wildfire_sums_ourdasy, grandtotal_classes)
wildfire_grandtotals_epadasy <- map2(wildfire_polygons_epadasycrs, wildfire_sums_epadasy, grandtotal_classes)
wildfire_grandtotals_huang <- map2(wildfire_polygons_huangcrs, wildfire_sums_huang, grandtotal_classes)
wildfire_grandtotals_fbpop <- map2(wildfire_polygons_fbpopcrs, wildfire_sums_fbpop, grandtotal_classes)

wildfire_grandtotals_blockgroups <- map(wildfire_sums_blockgroups, function(dat) {
  dat %>%
    rename(env_class = Var1) %>%
    group_by(env_class) %>%
    summarize(pop = sum(pop))
})

make_long_data <- function(grandtotals, estimate_name) {
  countydata %>%
    as_tibble %>%
    mutate(estimate = estimate_name) %>%
    mutate(listcol = grandtotals) %>%
    tidyr::unnest(cols = listcol)
}

wildfire_risks <- rbind(
  make_long_data(wildfire_grandtotals_ourdasy, 'our dasymetric'),
  make_long_data(wildfire_grandtotals_epadasy, 'EPA dasymetric'),
  make_long_data(wildfire_grandtotals_epadasy, 'Huang et al. population grid'),
  make_long_data(wildfire_grandtotals_fbpop, 'Facebook pop map'),
  make_long_data(wildfire_grandtotals_blockgroups, 'Census block group equal weighting')
)
```

```{r, echo = FALSE, eval = FALSE}
write_csv(wildfire_risks, '/nfs/qread-data/DASY/wildfire_risk_totals.csv')
```

## Process data some more

Get rid of classes 6 and 7 which are unknown. Then normalize the totals so that we can compare them among counties with different populations. Also order the counties by population.

```{r}
pop_order <- c('Gove, KS', 'Quitman, MS', 'Coos, NH', 'Gallatin, MT', 'La Salle, IL', 'Durham, NC', 'Ada, ID', 'Anne Arundel, MD', 'Denver, CO', 'New York, NY', 'Los Angeles, CA')

wildfire_risks <- wildfire_risks %>%
  mutate(county_name = factor(glue('{county_name}, {state_name}'), levels = pop_order)) %>%
  filter(!env_class %in% 6:7, !is.na(env_class)) %>%
  group_by(estimate, county_name) %>%
  mutate(pop_proportion = pop/sum(pop))
```

## Make a figure

```{r out.width="100%"}
ggplot(wildfire_risks, aes(x = env_class, y = pop_proportion, fill = estimate)) +
  geom_col(position = 'dodge') +
  facet_wrap(~ county_name) +
  scale_x_discrete(name = 'Wildfire risk', labels = c('very low', 'low', 'med', 'high', 'very high')) +
  scale_y_continuous(name = 'Population proportion', limits = c(0, 1), expand = expansion(mult = c(0, 0.01))) +
  scale_fill_brewer(palette = 'Dark2', labels = function(x) stringr::str_wrap(x, width = 15)) +
  theme_bw() +
  theme(strip.background = element_blank(),
        legend.position = c(0.9, 0.1),
        axis.text.x = element_text(size = rel(0.75)))
```

Some initial thoughts on the figure:

There seems to be at least a modest difference in inference depending on population estimation method. For instance, in Denver County, our dasymetric estimate puts more people in the "very low" and "medium" categories at the expense of "low", relative to an equal weighting approach. But in Los Angeles County, our dasymetric estimate seems to put more people in the lower risk categories than the equal-weighting approach. Obviously for the areas where fire risk is very low across the board, it doesn't matter. Also notice that the four dasymetric-style estimates all provide very similar inference to one another, but they all tend to be different from the naive equal-weighting approach.

Final note: there are many NA or unclassified pixels in the wildfire raster for some of the counties, which aren't included in the figure. I believe that is why New York County (Manhattan) is shown as having uniformly high wildfire risk. Most of the pixels there are unclassified and maybe just a few pixels have values there. It might not be the best example to include in the final figure.

## Changelog

- 6 July 2021: original version
- 14 July 2021: modified to use new dasy rasters (change file paths)
- 29 July 2021: new version with both wildfire risk and flood risk maps
