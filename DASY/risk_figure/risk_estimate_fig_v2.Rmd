---
title: "Risk estimate figure code: V2.0"
author: "Quentin D. Read"
date: "7/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This document has all the code to make the data underlying the "validation" or inference figure, which will give a visual answer to the question of how much using dasymetric population estimates actually changes our inference. This is the second version, which includes two different case studies: wildfire risk in counties in the western USA, and flood risk in coastal counties. For the first example I am using the Forest Service's wildfire hazard map that classifies the contiguous USA into 5 fire risk categories, at 270-meter pixel size. For the second example I am using the flood risk maps provided by FEMA, which include the flood water surface elevation (WSE) for a 100-year flood event at a 10-meter pixel resolution.

The procedure is:

- take a stratified random sample of the counties of interest, stratified by population.
- clip population rasters and environmental rasters to study area boundaries (we're using a few selected counties). 
- convert the environmental rasters to polygons, merging all adjacent pixels with the same class
- for each polygon, sum the dasymetric population pixels in that polygon to get numbers of people in that risk category
- sum across risk classes to get a histogram for each county
- compare the histograms for each county side-by-side to see if there is a difference in inference between population estimates

The population estimates we're using for now are:

- Our dasymetric estimate
- EPA dasymetric estimate
- Facebook population estimate
- Huang et al. population grid
- Census block groups with population spread equally across block group

*Note*: some of this code is not run when this notebook is rendered because it takes a long time. I ran it earlier and saved the output to load when the notebook is rendered.

## Stratified random sample of counties

I took the SRS of counties in a separate script. It isn't too important which counties are used because this is just illustrative. For the wildfire risk, I took all counties in the eleven Western states (WA, OR, CA, ID, NV, MT, WY, CO, UT, AZ, NM) and divided them up by population quintile (5 groups). I selected three random counties from each quintile. I did the same for flood risk except I used only coastal states in the lower 48 (both Atlantic and Pacific coasts). I had to sample quite a few counties to get to 10 counties that actually had flood risk rasters available from FEMA, so they are not perfectly divided among the quintiles. But there is a decent spread of population.

The wildfire risk map is available for the entire contiguous USA but the FEMA flood risk maps are only available for some of the counties. I went to the FEMA site and searched each county manually to get the data. Many coastal counties do not have the water surface elevation for 100-year flood event data product available in GeoTIFF format --- it's also not a random sample because there are a good number of coastal states that do not have the product for any county. After going through a lot of counties, I got data for ten counties which should be plenty for our case study.

What counties are we using?

```{r}
counties_wildfire <- read.csv('/nfs/qread-data/DASY/counties_wildfire.csv', colClasses = 'character')
counties_flood <- read.csv('/nfs/qread-data/DASY/counties_flood_allfematifs.csv', colClasses = 'character')
```

```{r, echo = FALSE}
knitr::kable(counties_wildfire[,1:2], col.names = c('County', 'State'), caption = 'Wildfire risk counties')

counties_flood$county_name <- paste(counties_flood$county_name, 'County')

knitr::kable(counties_flood[,2:1], col.names = c('County', 'State'), caption = 'Flood risk counties')
```

## Generate separate shapefiles for each target county

This makes it a lot easier to do operations on each county separately. We also project them to lat-long geographic coordinate system for use with the Facebook raster.

```{r, message = FALSE, warning = FALSE}
library(stars)
library(sf)
library(purrr)
library(dplyr)
library(glue)
library(tidycensus)
library(readr)
library(ggplot2)
library(gdalUtils)
library(rslurm)
library(tidyr)
library(grid)
library(gridExtra)

counties_wildfire <- counties_wildfire %>% mutate(fips = paste0(state_code, county_code))

```

Here note that the FIPS code has to be put in single quotes for the SQL WHERE. In contrast, when running from the command line I had to put it in double quotes. That took a lot of trial and error to figure out!

```{r, eval = FALSE}
gpkgpath <- "/nfs/qread-data/DASY/countybounds"
raster_proj <- gdalsrsinfo('/nfs/qread-data/raw_data/facebookpop/population_usa_2019-07-01.vrt', as.CRS = TRUE)

all_fips <- bind_rows(counties_flood, counties_wildfire) %>%
  select(state_code, county_code)

pwalk(all_fips, function(state_code, county_code) {
  fips <- paste0(state_code, county_code)
  # Extract only one county from the all county geopackage
  ogr2ogr(src_datasource_name = '/nfs/qread-data/raw_data/landuse/USA/USA_county_2014_aea.gpkg',
          dst_datasource_name = glue('{gpkgpath}/county{fips}.gpkg'),
          overwrite = TRUE, f = 'GPKG', where = glue('fips=\'{fips}\''))
  # Project the single county GPKG to the geographic CRS used by Facebook
  ogr2ogr(src_datasource_name = glue('{gpkgpath}/county{fips}.gpkg'),
          dst_datasource_name = glue('{gpkgpath}/county{fips}longlat.gpkg'),
          overwrite = TRUE, f  = 'GPKG', t_srs = raster_proj)
})
```

## Clip rasters to study area boundaries

Our dasymetric rasters and the flood risk rasters are already split up by county. So here it's just the wildfire raster, the EPA dasymetric raster, the Facebook raster, and the Huang et al. population grid that are being split up. Also ensure that our dasymetric raster matches the long-lat geographic CRS by creating a new version.

```{r, eval = FALSE}
tifpath <- "/nfs/qread-data/DASY/countyrasters"
gpkgpath <- "/nfs/qread-data/DASY/countybounds"

pwalk(all_fips, function(state_code, county_code) {
  fips <- paste0(state_code, county_code)
  # Wildfire
  gdalwarp(srcfile = '/nfs/qread-data/DASY/whp/Data/whp2020_GeoTIF/whp2020_cls_conus.tif',
           dstfile = glue('{tifpath}/county{fips}_wildfire.tif'),
           overwrite = TRUE, crop_to_cutline = TRUE,
           cutline = glue('{gpkgpath}/county{fips}.gpkg'))
  # EPA Dasymetric
  gdalwarp(srcfile = '/nfs/qread-data/DASY/epadasy/dasymetric_us_20160208/dasymetric_us_20160208.tif',
           dstfile = glue('{tifpath}/county{fips}_epadasy.tif'),
           overwrite = TRUE, crop_to_cutline = TRUE,
           cutline = glue('{gpkgpath}/county{fips}.gpkg'))
  # Facebook Dasymetric
  gdalwarp(srcfile = '/nfs/qread-data/raw_data/facebookpop/population_usa_2019-07-01.vrt',
           dstfile = glue('{tifpath}/county{fips}_fblonglat.tif'),
           overwrite = TRUE, crop_to_cutline = TRUE,
           cutline = glue('{gpkgpath}/county{fips}longlat.gpkg'))
  # Huang Dasymetric
  gdalwarp(srcfile = '/nfs/qread-data/DASY/huang_grid/PopGrid.tif',
           dstfile = glue('{tifpath}/county{fips}_huangdasy.tif'),
           overwrite = TRUE, crop_to_cutline = TRUE,
           cutline = glue('{gpkgpath}/county{fips}.gpkg'))
  # Our dasymetric, crop to cutline with the longlat object
  gdalwarp(srcfile = glue('/nfs/qread-data/DASY/tifs/neon-dasy-{state_code}-{county_code}.tif'),
           dstfile = glue('{tifpath}/county{fips}_dasy.tif'),
           overwrite = TRUE, crop_to_cutline = TRUE, tr = c(30, 30),
           cutline = glue('{gpkgpath}/county{fips}longlat.gpkg'))
})
```

## Read data

Loop through and read the rasters for each county. 

```{r, message = FALSE, warning = FALSE, results = "hide"}
tifpath <- '/nfs/qread-data/DASY/countyrasters'
dasypath <- '/nfs/qread-data/DASY/tifs'

wildfire_rasters <- map(counties_wildfire$fips, ~ read_stars(glue('{tifpath}/county{.}_wildfire.tif')))
ourdasy_rasters_wf <- map(counties_wildfire$fips, ~ read_stars(glue('{dasypath}/neon-dasy-{substr(., 1, 2)}-{substr(., 3, 5)}.tif')))
epadasy_rasters_wf <- map(counties_wildfire$fips, ~ read_stars(glue('{tifpath}/county{.}_epadasy.tif')))
huang_rasters_wf <- map(counties_wildfire$fips, ~ read_stars(glue('{tifpath}/county{.}_huangdasy.tif')))
fbpop_rasters_wf <- map(counties_wildfire$fips, ~ read_stars(glue('{tifpath}/county{.}_fblonglat.tif')))

flood_rasters <- map(counties_flood$wse_filename, read_stars)
ourdasy_rasters_fl <- map(counties_flood$fips, ~ read_stars(glue('{dasypath}/neon-dasy-{substr(., 1, 2)}-{substr(., 3, 5)}.tif')))
epadasy_rasters_fl <- map(counties_flood$fips, ~ read_stars(glue('{tifpath}/county{.}_epadasy.tif')))
huang_rasters_fl <- map(counties_flood$fips, ~ read_stars(glue('{tifpath}/county{.}_huangdasy.tif')))
fbpop_rasters_fl <- map(counties_flood$fips, ~ read_stars(glue('{tifpath}/county{.}_fblonglat.tif')))
```

Also load the census block group population estimates and geographies. These will be used for the "naive equal weighting" population estimate. Note that all the flood rasters have a different coordinate reference system so the block group population polygons are transformed each to a different one.

```{r, message = FALSE, eval = FALSE}
get_blockgroup_pop <- function(FIPS, crs) {
 
  pop <- get_acs(geography = "block group", variables = "B01003_001", 
                 year = 2016, state= substr(FIPS, 1, 2), county = substr(FIPS, 3, 5), 
                 geometry = TRUE)   
  
  # Data QC: remove empty geometries from pop
  pop <- pop[!is.na(st_dimension(pop)), ]
  
  st_transform(pop, crs = crs)
}

census_api_key(readLines('/research-home/qread/censusapikey.txt')) 
crs_wildfire <- st_crs(wildfire_rasters[[1]])
blockgroup_pops_wf <- map(counties_wildfire$fips, get_blockgroup_pop, crs = crs_wildfire)

crs_flood <- map(flood_rasters, st_crs)
blockgroup_pops_fl <- map2(counties_flood$fips, crs_flood, get_blockgroup_pop)

saveRDS(blockgroup_pops_wf, file = '/nfs/qread-data/DASY/blockgroup_pops_wildfire.RDS')
saveRDS(blockgroup_pops_fl, file = '/nfs/qread-data/DASY/blockgroup_pops_flood.RDS')
```

```{r}
blockgroup_pops_wf <- readRDS('/nfs/qread-data/DASY/blockgroup_pops_wildfire.RDS')
blockgroup_pops_fl <- readRDS('/nfs/qread-data/DASY/blockgroup_pops_flood.RDS')
```

## "Polygonize" the environmental rasters

We will use `st_as_sf()` to convert the wildfire raster and flood raster (converted to binary) to polygons. I needed to parallelize this because of the high memory requirements.

```{r, eval = FALSE}
sjob_wild <- slurm_map(wildfire_rasters, st_as_sf, jobname = 'wf_to_poly', 
                       nodes = 2, cpus_per_node = 4,
                       as_points = FALSE, merge = TRUE)
wildfire_polygons <- get_slurm_out(sjob_wild)
cleanup_files(sjob_wild)

make_poly_and_raster <- function(wse_filename, fips) {
  flood_raster <- raster(wse_filename)
  file_name <- glue('/nfs/qread-data/DASY/countyrasters/county{fips}_floodbinary.tif')
  if (!file.exists(file_name)) {
    flood_notna_raster <- st_as_stars(!is.na(flood_raster))
    write_stars(flood_notna_raster, file_name)
  } else {
    flood_notna_raster <- read_stars(file_name)
  }
  flood_notna_poly <- st_as_sf(flood_notna_raster, as_points = FALSE, merge = TRUE)
  return(flood_notna_poly)
}

sjob_flood <- slurm_apply(make_poly_and_raster, counties_flood[,c('wse_filename', 'fips')],
                          pkgs = c('raster', 'stars', 'sf', 'glue'), jobname = 'fl_to_poly',
                          nodes = 4, 
                          slurm_options = list(partition = 'sesync', mem = '100gb'))

flood_polygons <- get_slurm_out(sjob_flood)
cleanup_files(sjob_flood)

saveRDS(wildfire_polygons, file = '/nfs/qread-data/DASY/wildfire_polygons.RDS')
saveRDS(flood_polygons, file = '/nfs/qread-data/DASY/flood_polygons.RDS')
```

Convert the polygons to each of the rasters' coordinate reference systems. Correct invalid geometries in the lat-long polygons.

```{r}
wildfire_polygons <- readRDS('/nfs/qread-data/DASY/wildfire_polygons.RDS')
flood_polygons <- readRDS('/nfs/qread-data/DASY/flood_polygons.RDS')

wildfire_polygons_ourdasycrs <- map(wildfire_polygons, st_transform, crs = st_crs(ourdasy_rasters_wf[[1]]))
wildfire_polygons_epadasycrs <- map(wildfire_polygons, st_transform, crs = st_crs(epadasy_rasters_wf[[1]]))
wildfire_polygons_huangcrs <- map(wildfire_polygons, st_transform, crs = st_crs(huang_rasters_wf[[1]]))
wildfire_polygons_fbpopcrs <- map(wildfire_polygons, st_transform, crs = st_crs(fbpop_rasters_wf[[1]])) %>%
  map(st_make_valid)

flood_polygons_ourdasycrs <- map(flood_polygons, st_transform, crs = st_crs(ourdasy_rasters_fl[[1]]))
flood_polygons_epadasycrs <- map(flood_polygons, st_transform, crs = st_crs(epadasy_rasters_fl[[1]]))
flood_polygons_huangcrs <- map(flood_polygons, st_transform, crs = st_crs(huang_rasters_fl[[1]]))
flood_polygons_fbpopcrs <- map(flood_polygons, st_transform, crs = st_crs(fbpop_rasters_fl[[1]])) %>%
  map(st_make_valid)

```


## Get population totals by polygon

Now for each of these polygons, sum up the number of individuals. Repeat for each of the population estimates. Again parallelize with rslurm.

```{r, eval = FALSE}
aggregate_by_poly <- function(i, type, suffix) {
  sums_ourdasy <- aggregate(get(paste0('ourdasy_rasters_', suffix))[[i]], get(paste0(type,'_polygons_ourdasycrs'))[[i]], FUN = sum, na.rm = TRUE) %>%
    st_as_sf() %>% st_drop_geometry()
  sums_epadasy <- aggregate(get(paste0('epadasy_rasters_', suffix))[[i]], get(paste0(type,'_polygons_epadasycrs'))[[i]], FUN = sum, na.rm = TRUE) %>%
    st_as_sf() %>% st_drop_geometry()
  sums_huang <- aggregate(get(paste0('huang_rasters_', suffix))[[i]], get(paste0(type,'_polygons_huangcrs'))[[i]], FUN = sum, na.rm = TRUE) %>%
    st_as_sf() %>% st_drop_geometry()
  sums_fbpop <- aggregate(get(paste0('fbpop_rasters_', suffix))[[i]], get(paste0(type,'_polygons_fbpopcrs'))[[i]], FUN = sum, na.rm = TRUE) %>%
    st_as_sf() %>% st_drop_geometry()
  cbind(sums_ourdasy, sums_epadasy, sums_huang, sums_fbpop) %>%
    setNames(c('our_dasy', 'epa_dasy', 'huang', 'fb'))
}

sjob_wild_sums <- slurm_apply(aggregate_by_poly, data.frame(i = 1:nrow(counties_wildfire)),
                              type = 'wildfire', suffix = 'wf',
                              global_objects = c('wildfire_polygons_ourdasycrs', 'wildfire_polygons_epadasycrs',
                                                 'wildfire_polygons_huangcrs', 'wildfire_polygons_fbpopcrs',
                                                 'ourdasy_rasters_wf', 'epadasy_rasters_wf',
                                                 'huang_rasters_wf', 'fbpop_rasters_wf'), 
                              jobname = 'agg_wf', nodes = 4, cpus_per_node = 2,
                              pkgs = c('stars', 'sf'))

wildfire_sums <- get_slurm_out(sjob_wild_sums)
cleanup_files(sjob_wild_sums)

sjob_flood_sums <- slurm_apply(aggregate_by_poly, data.frame(i = 1:nrow(counties_flood)),
                              type = 'flood', suffix = 'fl',
                              global_objects = c('flood_polygons_ourdasycrs', 'flood_polygons_epadasycrs',
                                                 'flood_polygons_huangcrs', 'flood_polygons_fbpopcrs',
                                                 'ourdasy_rasters_fl', 'epadasy_rasters_fl',
                                                 'huang_rasters_fl', 'fbpop_rasters_fl'), 
                              jobname = 'agg_fl', nodes = 4, cpus_per_node = 2,
                              pkgs = c('stars', 'sf'))

flood_sums <- get_slurm_out(sjob_flood_sums)
cleanup_files(sjob_flood_sums)

saveRDS(wildfire_sums, file = '/nfs/qread-data/DASY/sums_wildfire.RDS')
saveRDS(flood_sums, file = '/nfs/qread-data/DASY/sums_flood.RDS')
```

For the wildfire by block group population weighting, we will do it the opposite way (population polygon and wildfire raster). Tabulate wildfire pixels for each block group polygon, and convert to a matrix. Then divide the population of each block group among the classes based on the number of pixels in each class in each block group. 

```{r, eval = FALSE}
table_by_polygon <- function(env_raster, bg_pop_poly) {
  map(st_geometry(bg_pop_poly), ~ as.data.frame(table(st_crop(env_raster, .)[[1]], useNA = 'always')))
}

wildfire_tables_blockgroups <- map2(wildfire_rasters, blockgroup_pops_wf, table_by_polygon)

sum_classes_blockgroup <- function(pop_table, bg_poly) {
  map_dfr(1:length(pop_table), ~ data.frame(st_drop_geometry(bg_poly[., c('GEOID', 'estimate')]), pop_table[[.]])) %>%
    group_by(GEOID) %>%
    mutate(pop = estimate * Freq / sum(Freq))
}

wildfire_sums_blockgroups <- map2(wildfire_tables_blockgroups, blockgroup_pops_wf, sum_classes_blockgroup)

saveRDS(wildfire_sums_blockgroups, file = '/nfs/qread-data/DASY/sums_wildfire_blockgroups.RDS')
```

The method for flood sums by block group is different as well. We are using the polygons in both cases, and calculating the areas of intersection of block group and flood polygons.

```{r, eval = FALSE}
get_blockgroup_sums <- function(bg_pop, env_poly, env_crs) {
  env_poly <- st_make_valid(env_poly)
  bg_pop %>%
    st_transform(env_crs) %>%
    mutate(area = st_area(.)) %>%
    st_intersection(env_poly) %>%
    mutate(area_int = st_area(.))
}

sjob_flood_block <- slurm_apply(function(i) get_blockgroup_sums(blockgroup_pops_fl[[i]],
                                                                flood_polygons_ourdasycrs[[i]],
                                                                st_crs(flood_polygons_ourdasycrs[[1]])),
                                params = data.frame(i = 1:length(blockgroup_pops_fl)),
                                global_objects = c('get_blockgroup_sums', 'blockgroup_pops_fl', 'flood_polygons_ourdasycrs'),
                                jobname = 'floodblock', nodes = 4, cpus_per_node = 2,
                                pkgs = c('stars', 'sf', 'dplyr'))

flood_sums_blockgroups <- get_slurm_out(sjob_flood_block)
cleanup_files(sjob_flood_block)

saveRDS(flood_sums_blockgroups, file = '/nfs/qread-data/DASY/sums_flood_blockgroups.RDS')
```


## Get grand totals by class for each county

Read the data objects created with Slurm jobs back into R environment. Do some initial processing for the plot labels.

```{r}
wildfire_sums <- readRDS(file = '/nfs/qread-data/DASY/sums_wildfire.RDS')
flood_sums <- readRDS(file = '/nfs/qread-data/DASY/sums_flood.RDS')
wildfire_sums_blockgroups <- readRDS(file = '/nfs/qread-data/DASY/sums_wildfire_blockgroups.RDS')
flood_sums_blockgroups <- readRDS(file = '/nfs/qread-data/DASY/sums_flood_blockgroups.RDS')

wildfire_pop_totals <- map_dbl(blockgroup_pops_wf, ~ sum(.$estimate))
flood_pop_totals <- map_dbl(blockgroup_pops_fl, ~ sum(.$estimate))

counties_wildfire <- mutate(counties_wildfire, total_pop = wildfire_pop_totals)
counties_flood <- mutate(counties_flood, total_pop = flood_pop_totals)

# Ordered factor facet labels for counties
counties_wildfire <- counties_wildfire %>%
  mutate(total_pop = wildfire_pop_totals,
         county_label = glue('{county_name}, {state_name} ({as.character(as.integer(signif(total_pop, 3)))})'),
         county_label = factor(county_label, levels = county_label[order(total_pop)]))

counties_flood <- counties_flood %>%
  mutate(total_pop = flood_pop_totals,
         county_label = glue('{county_name}, {state_name} ({as.character(as.integer(signif(total_pop, 3)))})'),
         county_label = factor(county_label, levels = county_label[order(total_pop)]))
```

Calculate grand totals of the wildfire risk classes for each county.

```{r, message = FALSE}
grandtotal_classes <- function(env_poly, pop_sums) {
  data.frame(env_class = env_poly[[1]], pop_sums) %>%
    mutate(env_class = as.character(env_class)) %>%
    group_by(env_class) %>%
    summarize_all(sum, na.rm = TRUE)
}

wildfire_grandtotals <- map2(wildfire_polygons, wildfire_sums, grandtotal_classes)

wildfire_grandtotals_blockgroups <- map(wildfire_sums_blockgroups, function(dat) {
  dat %>%
    rename(env_class = Var1) %>%
    mutate(env_class = as.character(env_class)) %>%
    group_by(env_class) %>%
    summarize(blockgroup = sum(pop))
})

wildfire_grandtotals <- map2(wildfire_grandtotals, wildfire_grandtotals_blockgroups, full_join)

wildfire_grandtotals <- counties_wildfire %>% 
  mutate(listcol = wildfire_grandtotals) %>%
  unnest(cols = listcol)
```

Do the same for flood groups.

```{r, message = FALSE}
flood_grandtotals <- map2(flood_polygons, flood_sums, grandtotal_classes)

get_blockgroup_grandtotals <- function(env_sums) {
  env_sums %>%
    st_drop_geometry() %>%
    rename_with(~ 'env_class', ends_with('tif') | contains('layer')) %>%
    mutate(env_class = as.character(env_class)) %>%
    group_by(env_class) %>%
    summarize(blockgroup = as.numeric(sum(estimate * area_int/area)))
}

flood_grandtotals_blockgroups <- map(flood_sums_blockgroups, get_blockgroup_grandtotals)

flood_grandtotals <- map2(flood_grandtotals, flood_grandtotals_blockgroups, full_join)

flood_grandtotals <- counties_flood %>% 
  mutate(listcol = flood_grandtotals) %>%
  unnest(cols = listcol)
```

```{r, echo = FALSE, eval = FALSE}
write_csv(wildfire_grandtotals, '/nfs/qread-data/DASY/wildfire_grandtotals_finalfig.csv')
write_csv(flood_grandtotals, '/nfs/qread-data/DASY/flood_grandtotals_finalfig.csv')
save(list = c('wildfire_grandtotals', 'flood_grandtotals'), file = '/nfs/qread-data/DASY/grandtotals_finalfig.RData')
```


## Get population proportions

Divide the population in each risk category by the total population (totals for each estimation method may differ).

```{r}
wildfire_risks <- wildfire_grandtotals %>%
  group_by(county_name, state_name, state_code, county_code, fips, county_label) %>%
  mutate(across(c(our_dasy, epa_dasy, huang, fb, blockgroup), ~ ./sum(., na.rm = TRUE)))

flood_risks <- flood_grandtotals %>%
  group_by(county_name, state_name, state_code, county_code, fips, county_label) %>%
  mutate(across(c(our_dasy, epa_dasy, huang, fb, blockgroup), ~ ./sum(., na.rm = TRUE)))
```


## Make a figure

```{r out.width="100%", message = FALSE}
label_names <- c('This study', 'U.S. EPA', 'Huang et al.', 'Facebook', 'Block group area weighting')
estimate_labels <- data.frame(estimate = c('our_dasy', 'epa_dasy', 'huang', 'fb', 'blockgroup'),
                              estimate_label = factor(label_names, levels = label_names))

wildfire_risks_long <- wildfire_risks %>%
  filter(env_class %in% as.character(1:5)) %>%
  select(county_label, env_class, our_dasy, epa_dasy, huang, fb, blockgroup) %>%
  pivot_longer(c(our_dasy, epa_dasy, huang, fb, blockgroup), names_to = 'estimate', values_to = 'proportion') %>%
  left_join(estimate_labels)

ggplot(wildfire_risks_long, 
       aes(x = env_class, y = proportion, fill = estimate_label)) +
  geom_col(position = 'dodge') +
  facet_wrap(~ county_label) +
  scale_x_discrete(name = 'Wildfire risk', labels = c('very low', 'low', 'med', 'high', 'very high')) +
  scale_y_continuous(name = 'Population proportion', limits = c(0, 0.6), expand = expansion(mult = c(0, 0))) +
  scale_fill_brewer(name = 'Dasymetric method', palette = 'Dark2', labels = function(x) stringr::str_wrap(x, width = 15)) +
  theme_bw() +
  theme(strip.background = element_blank(),
        legend.position = c(0.9, 0.1),
        axis.text.x = element_text(size = rel(0.75)))
```

```{r out.width="100%"}
flood_risks_long <- flood_risks %>%
  select(county_label, env_class, our_dasy, epa_dasy, huang, fb, blockgroup) %>%
  pivot_longer(c(our_dasy, epa_dasy, huang, fb, blockgroup), names_to = 'estimate', values_to = 'proportion') %>%
  left_join(estimate_labels)

ggplot(flood_risks_long, 
       aes(x = env_class, y = proportion, fill = estimate_label)) +
  geom_col(position = 'dodge') +
  facet_wrap(~ county_label) +
  scale_x_discrete(name = 'Flood risk', labels = c('not at risk', 'at risk')) +
  #scale_y_continuous(name = 'Population proportion', limits = c(0, 0.6), expand = expansion(mult = c(0, 0))) +
  scale_fill_brewer(name = 'Dasymetric method', palette = 'Dark2', labels = function(x) stringr::str_wrap(x, width = 15)) +
  theme_bw() +
  theme(strip.background = element_blank(),
        legend.position = c(0.9, 0.1),
        axis.text.x = element_text(size = rel(0.75)))
```

## Final figure, combining five counties for each risk type

This figure will show five counties for each risk type that illustrate the point best. We can also boil down the categories so that there is only one category per figure. For wildfire, it will be proportion of the population at medium or greater risk, and for flood it will be proportion of the population with >0 depth in the WSE 1% flood zone.

```{r, out.width="100%", message = FALSE}
counties_use_wf <- counties_wildfire$county_label[counties_wildfire$fips %in% c('49009', '35057', '49013', '53007', '04013')]
wildfire_risks_reduced <- wildfire_risks_long %>%
  filter(env_class %in% as.character(3:5), county_label %in% counties_use_wf) %>%
  group_by(county_label, estimate, estimate_label) %>%
  summarize(proportion = sum(proportion))

counties_use_fl <- counties_flood$county_label[counties_flood$fips %in% c('24019', '13029', '12035', '01003', '24003')]
flood_risks_reduced <- flood_risks_long %>%
  filter(env_class %in% '1', county_label %in% counties_use_fl) 

p_wf <- ggplot(wildfire_risks_reduced, aes(x = estimate_label, y = proportion, fill = estimate_label)) +
  geom_col() +
  facet_wrap(~ county_label, nrow = 1) +
  scale_y_continuous(name = 'Population at risk from wildfire', expand = expansion(mult = c(0, 0.01)), labels = scales::percent_format(accuracy = 1)) +
  scale_fill_brewer(name = 'Dasymetric method', palette = 'Dark2', labels = function(x) stringr::str_wrap(x, width = 15)) +
  theme_bw() +
  theme(strip.background = element_blank(),
        legend.position = c(0.9, 0.6),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.x = element_blank(),
        panel.grid = element_blank())

p_fl <- ggplot(flood_risks_reduced, aes(x = estimate_label, y = proportion, fill = estimate_label)) +
  geom_col() +
  facet_wrap(~ county_label, nrow = 1) +
  scale_y_continuous(name = 'Population at risk from flooding', expand = expansion(mult = c(0, 0.01)), labels = scales::percent_format(accuracy = 1)) +
  scale_fill_brewer(name = 'Dasymetric method', palette = 'Dark2', labels = function(x) stringr::str_wrap(x, width = 15)) +
  theme_bw() +
  theme(strip.background = element_blank(),
        legend.position = 'none',
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.x = element_blank(),
        panel.grid = element_blank())

p_both <- gtable_rbind(ggplotGrob(p_wf), ggplotGrob(p_fl))

grid.draw(p_both)

png('/nfs/qread-data/DASY/figs/wildfire_flood_figure.png', height = 6, width = 12, res = 300, units = 'in')
grid.draw(p_both)
dev.off()
```

I think the figure illustrates that there are reasonably big discrepancies among the methods. Maybe you could qualitatively say that the percent discrepancies are a little bigger for the smaller counties. You can also express it as absolute numbers of people.

## Changelog

- 6 July 2021: original version
- 14 July 2021: modified to use new dasy rasters (change file paths)
- 29 July 2021: new version with both wildfire risk and flood risk maps
